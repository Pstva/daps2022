{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8631761",
   "metadata": {},
   "source": [
    "# Churn prediction\n",
    "\n",
    "**Customer churn/attrition** -  **the percentage of customers that stop using a company's products or services.**\n",
    "It is one of the most important metrics for a business, as it usually costs more to acquire new customers than it does to retain existing ones.\n",
    "\n",
    "By using Survival Analysis, not only companies can predict if customers are likely to stop doing business but also when that event might happen.\n",
    "\n",
    "based on the tutorial https://square.github.io/pysurvival/tutorials/churn.html\n",
    "\n",
    "## Data\n",
    "Today, we will try to predict whether a customer will change telecommunications provider called Telco, data can be loaded here: https://www.kaggle.com/datasets/blastchar/telco-customer-churn\n",
    "\n",
    "The dataset includes information about:\n",
    "\n",
    "+ Customers who left within the last month – the column is called Churn\n",
    "+ Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n",
    "+ Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n",
    "+ Demographic info about customers – gender, age range, and if they have partners and dependents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077cb9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d3cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c8b45c",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "+ Selecting necessary columns\n",
    "+ One-hot encoding\n",
    "+ Checking NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d81df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no duplicates\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not need customerID\n",
    "data = data.drop(columns=['customerID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed297cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea3e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no NAs :)\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561148b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    print(f\"{col}:\")\n",
    "    print(data[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a941543",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_binary = ['SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\n",
    "col_cat = ['MultipleLines', 'InternetService','OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
    "       'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - No, 1 - Yes\n",
    "data[col_binary] = data[col_binary].applymap(lambda x: 0 if x == \"No\" else 1)\n",
    "# 0 -Male, 1 - Female\n",
    "data['gender'] = data['gender'].apply(lambda x: 0 if x == \"Male\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding for categorical with more than 2 categories\n",
    "pd.get_dummies(data[col_cat], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b856a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = [x for x in data.columns if x not in col_binary and x not in col_cat and x != 'gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numerical].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934995c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    try:\n",
    "        x = float(data['TotalCharges'][i])\n",
    "    except ValueError:\n",
    "        print(i,data['TotalCharges'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data['gender'], data[col_binary], pd.get_dummies(data[col_cat], drop_first=True), data[numerical]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da666f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba21f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['TotalCharges'] != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TotalCharges'] = data['TotalCharges'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numerical].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c07a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e17e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2c664",
   "metadata": {},
   "source": [
    "## Preparations for modelling\n",
    "\n",
    "Let's split our data to train and test. We will train our model on train, and then calculate metrics on test for model evaluation.\n",
    "\n",
    "\n",
    "**NOTE:** If you also want to tune some hyperparameters of the model, you need to split your data to 3 parts: train, validation and test. So, you train all your models on train, compare the metrics on validation set for different parameters, choose the best parameters according to the validation set. And after that test the final model on test set. This will help to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N = data.shape[0]\n",
    "index_train, index_test = train_test_split(range(N), test_size = 0.35)\n",
    "data_train = data.loc[index_train].reset_index(drop = True)\n",
    "data_test  = data.loc[index_test].reset_index(drop = True)\n",
    "\n",
    "features = [x for x in data.columns if x != 'Churn' and x != 'tenure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad088ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSurvivalForest needs y as the array in som structured way\n",
    "# list with pairs (churn, tenure)\n",
    "churn_tenure_train = [(e1,e2) for e1,e2 in np.array(data_train[[\"Churn\", \"tenure\"]])]\n",
    "\n",
    "# to numpy array\n",
    "churn_tenure_train = np.array(churn_tenure_train, dtype=[('Status', '?'), ('Survival_in_days', '<f8')])\n",
    "\n",
    "\n",
    "# the same for test\n",
    "\n",
    "churn_tenure_test = [(e1,e2) for e1,e2 in np.array(data_test[[\"Churn\", \"tenure\"]])]\n",
    "churn_tenure_test = np.array(churn_tenure_test, dtype=[('Status', '?'), ('Survival_in_days', '<f8')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da36a8a3",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "\n",
    "### Random Survival Forest \n",
    "\n",
    "Article about this method: https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-3/Random-survival-forests/10.1214/08-AOAS169.full\n",
    "\n",
    "Shorter article: https://www.randomforestsrc.org/articles/survival.html\n",
    "\n",
    "\n",
    "tutorial: https://scikit-survival.readthedocs.io/en/stable/user_guide/random-survival-forest.html\n",
    "\n",
    "\n",
    "\n",
    "Works with the right-censored data (if only the lower limit l for the true event time T is known such that T > l, this is called right censoring. Right censoring will occur, for example, for those subjects whose birth date is known but who are still alive when they are lost to follow-up or when the study ends.)\n",
    "\n",
    "\n",
    "As in the classical Random Forest, here in each tree the node is separated into two nodes using some criteria. In Random Forest, it is usually Gini coefficient or entropy (for classification). Here we use different function. A node maximizes survival difference between its children. \n",
    "\n",
    "It uses the Nelson–Aalen estimator (as the cumulative hazard function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a8b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-survival\n",
    "#there can be an error about the conflict with scikit-learn package\n",
    "#! pip install -U scikit-learn==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387737d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa07bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf = RandomSurvivalForest(n_estimators=1000,\n",
    "                           min_samples_split=10,\n",
    "                           min_samples_leaf=15,\n",
    "                           n_jobs=-1,\n",
    "                           random_state=10)\n",
    "rsf.fit(data_train[features], churn_tenure_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3616c64f",
   "metadata": {},
   "source": [
    "## Evaluating on test data\n",
    "\n",
    "(from the original article) **Prediction error**. To estimate prediction error, we use Harrell’s concordance index [Harrell et al. (1982)]. The C-index (concordance index) is related\n",
    "to the area under the ROC curve [Heagerty and Zheng (2005)]. \n",
    "\n",
    "It estimates the\n",
    "**probability that, in a randomly selected pair of cases, the case that fails first had a\n",
    "worst predicted outcome.** The interpretation of the C-index as a misclassification\n",
    "probability is attractive, and is one reason we use it for prediction error. Another attractive feature is that, unlike other measures of survival performance, the C-index\n",
    "does not depend on a single fixed time for evaluation. The C-index also specifically\n",
    "accounts for censoring.\n",
    "\n",
    "\n",
    "\n",
    "Prediction error = 1 - Concordance index\n",
    "\n",
    "If Prediction error == Concordance index == 0.5 5 indicates prediction no better than random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a concordance index \n",
    "rsf.score(data_test[features], churn_tenure_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece54fc",
   "metadata": {},
   "source": [
    "## Predicting\n",
    "\n",
    "For prediction, a sample is dropped down each tree in the forest until it reaches a terminal node. Data in each terminal is used to non-parametrically estimate the survival and cumulative hazard function using the Kaplan-Meier and Nelson-Aalen estimator, respectively. In addition, a risk score can be computed that represents the expected number of events for one particular terminal node. The ensemble prediction is simply the average across all trees in the forest.\n",
    "\n",
    "Let’s predict churn for a several customers from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_customers = data.sample(n=5, random_state=10)\n",
    "test_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bdca2",
   "metadata": {},
   "source": [
    "Algorithm predicts a risk score for each observation (in the article it's called mortality).\n",
    "\n",
    "A risk score represents the expected number of events for one particular terminal node. This estimates **the number of deaths expected if all cases were similar to X** (so all observations go to this node). \n",
    "\n",
    "We can see at this number as a risk score of churn - the bigger the value, the more the risk that this customer will leave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rsf.predict(test_customers[features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rsf.predict(data_test[features]), bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a9111",
   "metadata": {},
   "source": [
    "We can have a more detailed insight by considering the **predicted survival function**. \n",
    "\n",
    "\n",
    "$$S(t|{\\bf X})=\\mathbb{P}\\{T^o> t|{\\bf X}\\}$$ $T^o$ - survival event time\n",
    "\n",
    "So, this is the probablity that at a given time t, the person is alive (death would be at time $T^o$ which is bigger then t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e790f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = rsf.predict_survival_function(test_customers[features], return_array=True)\n",
    "\n",
    "for i, s in enumerate(surv):\n",
    "    plt.step(rsf.event_times_, s, where=\"post\", label=str(i))\n",
    "plt.ylabel(\"Survival probability\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f974294",
   "metadata": {},
   "source": [
    "Alternatively, we can also plot the predicted **cumulative hazard function.**\n",
    "\n",
    "\n",
    "$$F(u|{\\bf X})=\\mathbb{P}\\{T^o\\le u|{\\bf X}\\}.$$\n",
    "\n",
    "\n",
    "where $u$ is some time value.\n",
    "\n",
    "The probablity, that a person will not survive after time $u$ (a person will die at time $u$ or earlier)\n",
    "\n",
    "\n",
    "The above two functions are calculated in the forest with summing up the values of deaths/individuals at risk for each node/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ba297",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = rsf.predict_cumulative_hazard_function(test_customers[features], return_array=True)\n",
    "\n",
    "for i, s in enumerate(surv):\n",
    "    plt.step(rsf.event_times_, s, where=\"post\", label=str(i))\n",
    "plt.ylabel(\"Cumulative hazard\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f814ec",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Here we use permutation-based feature importance.\n",
    "\n",
    "Permutation feature importance is a model inspection technique that can be used for any fitted estimator when the data is tabular. This is especially useful for non-linear or opaque estimators. The permutation feature importance is defined to be **the decrease in a model score when a single feature value is randomly shuffled**. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. This technique benefits from being model agnostic and can be calculated many times with different permutations of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(\n",
    "    rsf, data_test[features], churn_tenure_test, n_repeats=15, random_state=10)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {k: result[k] for k in (\"importances_mean\", \"importances_std\",)},\n",
    "    index=data_test[features].columns).sort_values(by=\"importances_mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ef651",
   "metadata": {},
   "source": [
    "## Task for you\n",
    "\n",
    "**Deadline**: 04.10.2022 12:00, my e-mail: aspestova@hse.ru.\n",
    "\n",
    "!!! Send your solution in **html** format, please\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551794d",
   "metadata": {},
   "source": [
    "1. Take any other model suitable for Survival Analysis. For example, it can be from **slkearn-survival** package (https://scikit-survival.readthedocs.io/en/stable/user_guide/index.html) or **pysurvival** (the example of a function with the link to the documentation is given below). In a couple of words describe how the model differ from the model we used on the practice.\n",
    "\n",
    "2. Train the model on the train data. Compare the model results with those obtained on the practice (by concordance index).\n",
    "\n",
    "(You can use default parameters  or to tune the hyperparameters, but if you do so, split the data into 3 parts. And compare your final model and the model from the practice only on test set, tuning of hyperparameters should be done in validation set!)\n",
    "\n",
    "**Note**: if you use some other package (not sklearn) and it doesn't calculate the concordance index for you, then you can use the function from sklearn: https://scikit-survival.readthedocs.io/en/stable/api/generated/sksurv.metrics.concordance_index_censored.html. The first ans the second arguments are churn and tenure from test data, the third one - predictions of risk score by your model. **pysurvival** has method for calculation this index!\n",
    "\n",
    "3. Compute feature importance. **pysurvival** also can do it. Compare the results with the feature importances from the model we built on the practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b58c0",
   "metadata": {},
   "source": [
    "### Conditional Survival Forest \n",
    "\n",
    "**The Conditional Survival Forest model** was developed by [Wright et al. in 2017](https://arxiv.org/pdf/1605.03391.pdf) to improve the Random Survival Forest training, whose objective function tends to favor splitting variables with many possible split points.\n",
    "\n",
    "Tutorial on it: https://square.github.io/pysurvival/tutorials/churn.html\n",
    "\n",
    "You can use this library and this model (or some other if you find it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
